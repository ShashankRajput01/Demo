{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-qGv4sNBhM0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import pyttsx3\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Input text - to summarize\n",
        "def summarize(text1):\n",
        "    text = text1\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Creating a frequency table to keep the\n",
        "    # score of each word\n",
        "\n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    # Creating a dictionary to keep the score\n",
        "    # of each sentence\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentenceValue = dict()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word, freq in freqTable.items():\n",
        "            if word in sentence.lower():\n",
        "                if sentence in sentenceValue:\n",
        "                    sentenceValue[sentence] += freq\n",
        "                else:\n",
        "                    sentenceValue[sentence] = freq\n",
        "\n",
        "\n",
        "\n",
        "    sumValues = 0\n",
        "    for sentence in sentenceValue:\n",
        "        sumValues += sentenceValue[sentence]\n",
        "\n",
        "    # Average value of a sentence from the original text\n",
        "\n",
        "    try:\n",
        "        average = int(sumValues/(len(sentenceValue)+1))\n",
        "    except:\n",
        "        temp1=\"\"\n",
        "\n",
        "    # Storing sentences into our summary.\n",
        "    summary = ''\n",
        "    for sentence in sentences:\n",
        "        if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)):\n",
        "            summary += \" \" + sentence\n",
        "    return(summary)\n",
        "\n",
        "#article_list=[]\n",
        "#n= int(input(\"Enter the number of articles: \"))\n",
        "para=(input(\"enter article: \"))\n",
        "res=\"\"\n",
        "#fres=\"\"\n",
        "\n",
        "res=summarize(para)\n",
        "\n",
        "print()\n",
        "print(\"Detailed summary: \",res)\n",
        "\n",
        "\n",
        "def solution(paragraph,question):\n",
        "\n",
        "    encoding = tokenizer.encode_plus(text=question,text_pair=paragraph)\n",
        "\n",
        "    inputs = encoding['input_ids']  #Token embeddings\n",
        "    sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n",
        "    start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n",
        "    start_index = torch.argmax(start_scores)\n",
        "\n",
        "    end_index = torch.argmax(end_scores)\n",
        "\n",
        "    answer = ' '.join(tokens[start_index:end_index+1])\n",
        "\n",
        "    corrected_answer = ''\n",
        "\n",
        "    for word in answer.split():\n",
        "\n",
        "        #If it's a subword token\n",
        "        if word[0:2] == '##':\n",
        "            corrected_answer += word[2:]\n",
        "        else:\n",
        "            corrected_answer += ' ' + word\n",
        "\n",
        "    print(\"Here is the answer:  \",corrected_answer)\n",
        "\n",
        "\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad',return_dict=False)\n",
        "\n",
        "#Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "text = res\n",
        "question = input(\"Ask Question: \")\n",
        "while True:\n",
        "    solution(text,question)\n",
        "\n",
        "    flag = True\n",
        "    flag_N = False\n",
        "\n",
        "    while flag:\n",
        "        response = input(\"\\nDo you want to ask another question based on this text (Y/N)? \")\n",
        "        if response[0] == \"Y\":\n",
        "            question = input(\"Ask Question: \")\n",
        "            flag = False\n",
        "        elif response[0] == \"N\":\n",
        "            print(\"\\nBye see you again!\")\n",
        "            flag = False\n",
        "            flag_N = True\n",
        "\n",
        "    if flag_N == True:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "Z0_bbQDkBvzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyttsx3"
      ],
      "metadata": {
        "id": "JEP-ehouB8kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "L12WO81XCL3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "EnX1mEzZCUed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import pyttsx3\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Input text - to summarize\n",
        "def summarize(text1):\n",
        "    text = text1\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Creating a frequency table to keep the\n",
        "    # score of each word\n",
        "\n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    # Creating a dictionary to keep the score\n",
        "    # of each sentence\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentenceValue = dict()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word, freq in freqTable.items():\n",
        "            if word in sentence.lower():\n",
        "                if sentence in sentenceValue:\n",
        "                    sentenceValue[sentence] += freq\n",
        "                else:\n",
        "                    sentenceValue[sentence] = freq\n",
        "\n",
        "\n",
        "\n",
        "    sumValues = 0\n",
        "    for sentence in sentenceValue:\n",
        "        sumValues += sentenceValue[sentence]\n",
        "\n",
        "    # Average value of a sentence from the original text\n",
        "\n",
        "    try:\n",
        "        average = int(sumValues/(len(sentenceValue)+1))\n",
        "    except:\n",
        "        temp1=\"\"\n",
        "\n",
        "    # Storing sentences into our summary.\n",
        "    summary = ''\n",
        "    for sentence in sentences:\n",
        "        if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)):\n",
        "            summary += \" \" + sentence\n",
        "    return(summary)\n",
        "\n",
        "#article_list=[]\n",
        "#n= int(input(\"Enter the number of articles: \"))\n",
        "para=(input(\"enter article: \"))\n",
        "res=\"\"\n",
        "#fres=\"\"\n",
        "\n",
        "res=summarize(para)\n",
        "\n",
        "print()\n",
        "print(\"Detailed summary: \",res)\n",
        "\n",
        "\n",
        "def solution(paragraph,question):\n",
        "\n",
        "    encoding = tokenizer.encode_plus(text=question,text_pair=paragraph)\n",
        "\n",
        "    inputs = encoding['input_ids']  #Token embeddings\n",
        "    sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n",
        "    start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n",
        "    start_index = torch.argmax(start_scores)\n",
        "\n",
        "    end_index = torch.argmax(end_scores)\n",
        "\n",
        "    answer = ' '.join(tokens[start_index:end_index+1])\n",
        "\n",
        "    corrected_answer = ''\n",
        "\n",
        "    for word in answer.split():\n",
        "\n",
        "        #If it's a subword token\n",
        "        if word[0:2] == '##':\n",
        "            corrected_answer += word[2:]\n",
        "        else:\n",
        "            corrected_answer += ' ' + word\n",
        "\n",
        "    print(\"Here is the answer:  \",corrected_answer)\n",
        "\n",
        "\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad',return_dict=False)\n",
        "\n",
        "#Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "text = res\n",
        "question = input(\"Ask Question: \")\n",
        "while True:\n",
        "    solution(text,question)\n",
        "\n",
        "    flag = True\n",
        "    flag_N = False\n",
        "\n",
        "    while flag:\n",
        "        response = input(\"\\nDo you want to ask another question based on this text (Y/N)? \")\n",
        "        if response[0] == \"Y\":\n",
        "            question = input(\"Ask Question: \")\n",
        "            flag = False\n",
        "        elif response[0] == \"N\":\n",
        "            print(\"\\nBye see you again!\")\n",
        "            flag = False\n",
        "            flag_N = True\n",
        "\n",
        "    if flag_N == True:\n",
        "        break"
      ],
      "metadata": {
        "id": "60twIm9wB4C0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}